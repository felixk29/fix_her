{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEVER FORGET\n",
    "  1. test all functions you made\n",
    "  1. sign of anything wrong, dont ignore it,  \n",
    "  1. write thesis report \n",
    "  1. read papers\n",
    "\n",
    "## Todos:\n",
    "  1. maybe rnd exploration\n",
    "  1. write thesis\n",
    "  1. include zero shot in thesis background\n",
    "  1. implement method that just goes somewhere before exploiting (random) as comparison\n",
    "  1. adapt buffers to actually handle vecenv\n",
    "  1. maybe add a tracker how many steps are taken by the uvf (maybe even with max_steps) (with euclidean distance between start and end obs, and end and goal obs)\n",
    "  1. max-step for uvf \n",
    "  1. profile code (cprofile, snakevis or smth for visualizing)\n",
    "  1. try out 2x channels (64) for multinputcnn :check:\n",
    "\n",
    "\n",
    "### Experiments:\n",
    "- [ ] initialize with optimal trajectory to figure out how much is exploration and how much is generalization \n",
    "  - just fill replay buffer with optimal trajectory (is in repo), otherwise use eval callback to figure out perentage of visited state and compare, if the lines are the same between tpdqn and normal dqn, means exploration is not the difference \n",
    "  - could also do a callback that keeps track of diversity of replaybuffer state\n",
    "\n",
    "## Pseudopseudocode\n",
    "### Rollout\n",
    " 1. episode starts \n",
    " 1. if random() < chance: continue; else goto 5. \n",
    " 1. sample buffer for states, choose highest RND value state x, \n",
    " 1. utilize uvf to go to state x, until value function doesn't increase further\n",
    " 1. use main agent to try to go to goal\n",
    "### Training\n",
    " 1. for main agent use standard dual dqn\n",
    " 1. for uvf utilize HER\n",
    "\n",
    "## New questions\n",
    " - How to improve: \n",
    "   - part of the problem is very limited amount of real steps, forcing it to go more steps at the start might help? didnt help\n",
    "   - forcing it to continue when both uvf values (most recent and old one) are negative\n",
    "   - neither position oriented nor only context drawn, matters\n",
    "   - exploration of uvf currently is max(exploration_rate of agen, 0.2)\n",
    "\n",
    "\n",
    "## Answered Questions: \n",
    "\n",
    " - How many states, (6240 prob not as teleport reaches 119%): doesnt really matter\n",
    " - stacking states is fine? (turn 4,9,9 into 8,9,9 for uvf)? any ideas for good parameters? maybe 64 channels, but thats it\n",
    " - reward function is fine?  yeah\n",
    " - Why is the reward not discounted? just cause succes more important than optimilatiy in our case\n",
    " - Is giving uvf a done and +1 reward if obs==goal good idea? can do, should use the reward\n",
    " - how to implement experiment discussed last week?  - not sure yet\n",
    " - can the idea of 2 networks be called a weird version of actor critic? (for reading/implementation purposes) \n",
    " - should uvf and main have the same buffer? (is it even feasible), 2 buffer would be easier, merging to one makes more sense when merging networks to one,\n",
    "   uvf should use both, main agent only itself, q functions on everything\n",
    " - should I even keep the chance thingy or for starters just do 100% chance runs? 0,50,100\n",
    " - should I rather extract target as coordinates, or just the corresponding layer (as one layer of input is just position)? also should I alter the input in this case?\n",
    " - should I change the max timesteps? as both targets are on the same environment, possible options: keep same, have different stepcounts for each, have one higher for both together - leave the same\n",
    " - what if uvf go to actual goal - doesnt matter\n",
    "\n",
    "monthly master meeting\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
