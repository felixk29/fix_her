{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talking points:\n",
    " - randomWalk shows improvements similar to tp, not as strong but 30 steps already helps, \n",
    " - implemented whole agent, did not work, so decided with max to first focus on uvf on itself,\n",
    " - made wrapper for environment so uvf can work on it (both explicit goal and state-goal possibility)\n",
    " - uvf kinda working now, made error when implementing by utilizing too little different goals during training\n",
    " - whole agent (call it hergo atm open to suggestions) still doesnt work, possible problems:  \n",
    "  - how to stop? (refer to heatmaps) uvf value decreasing doesn't work as that requires perfect uvf\n",
    "  - uvf agent needs to long to train and too big a buffer, kinda destroys the purpose of the reduced buffersize of agent (maybe I am missunderstanding something)\n",
    "  - currently also implemented that goal is drawn from same context as agent is in atm, (however that requires domain knowledge, which we wanted to avoid afaik)\n",
    " - will be gone one week in may, will continue work but by then things should look different already I think\n",
    " - couldnt find implementation of HER on PPO, \n",
    " - when midterm review? \n",
    "\n",
    "\n",
    "## NEVER FORGET\n",
    "  1. test all functions you made\n",
    "  1. sign of anything wrong, dont ignore it,  \n",
    "  1. write thesis report \n",
    "  1. read papers\n",
    "\n",
    "## Todos:\n",
    "  1. write thesis\n",
    "  1. maybe rnd exploration\n",
    "  1. analyse behaviour of agent video (maybe checkpointing during training to render of various stages during training) \n",
    "  1. test uvf with RND selection maybe? \n",
    "  1. doublecheck exploration function of hergo's uvf\n",
    "  1. let uvf run 30 steps regardless of everything else, maybe start high early and then reduce to like 30\n",
    "  1. doublecheck how sb3 uses imaginary goals in buffer (in sampling or in adding) - are created during sampling, otherwise wouldnt make much sense\n",
    "  1. PPO for UVF\n",
    "  1. look into decaying performance of goal oriented algorithms\n",
    "  1. look into buffersizes of UVF's (goal oriented algs)\n",
    "  1. look into on-policy HER algorithms\n",
    "  1. goal conditioned paper research\n",
    "  1. implement HER for PPO otherwise training not possible during full agen, and training should be enhanced that way during pure UVF\n",
    "  1. I dont understand her ratio at all at HerReplayBuffer line 90, tbh still dont really get it\n",
    "\n",
    "\n",
    "### Experiments:\n",
    "- [ ] initialize with optimal trajectory to figure out how much is exploration and how much is generalization \n",
    "  - just fill replay buffer with optimal trajectory (is in repo), otherwise use eval callback to figure out perentage of visited state and compare, if the lines are the same between tpdqn and normal dqn, means exploration is not the difference \n",
    "  - could also do a callback that keeps track of diversity of replaybuffer state\n",
    "\n",
    "## Pseudopseudocode\n",
    "### Rollout\n",
    " 1. episode starts \n",
    " 1. if random() < chance: continue; else goto 5. \n",
    " 1. sample buffer for states, choose highest RND value state x, \n",
    " 1. utilize uvf to go to state x, until value function doesn't increase further\n",
    " 1. use main agent to try to go to goal\n",
    "### Training\n",
    " 1. for main agent use standard dual dqn\n",
    " 1. for uvf utilize HER\n",
    "\n",
    "## New questions\n",
    " - How to improve: \n",
    "   - part of the problem is very limited amount of real steps, forcing it to go more steps at the start might help? didnt help\n",
    "   - forcing it to continue when both uvf values (most recent and old one) are negative\n",
    "   - neither position oriented nor only context drawn, matters\n",
    "   - exploration of uvf currently is max(exploration_rate of agen, 0.2)\n",
    "\n",
    "\n",
    "## Answered Questions: \n",
    "\n",
    " - How many states, (6240 prob not as teleport reaches 119%): doesnt really matter\n",
    " - stacking states is fine? (turn 4,9,9 into 8,9,9 for uvf)? any ideas for good parameters? maybe 64 channels, but thats it\n",
    " - reward function is fine?  yeah\n",
    " - Why is the reward not discounted? just cause succes more important than optimilatiy in our case\n",
    " - Is giving uvf a done and +1 reward if obs==goal good idea? can do, should use the reward\n",
    " - how to implement experiment discussed last week?  - not sure yet\n",
    " - can the idea of 2 networks be called a weird version of actor critic? (for reading/implementation purposes) \n",
    " - should uvf and main have the same buffer? (is it even feasible), 2 buffer would be easier, merging to one makes more sense when merging networks to one,\n",
    "   uvf should use both, main agent only itself, q functions on everything\n",
    " - should I even keep the chance thingy or for starters just do 100% chance runs? 0,50,100\n",
    " - should I rather extract target as coordinates, or just the corresponding layer (as one layer of input is just position)? also should I alter the input in this case?\n",
    " - should I change the max timesteps? as both targets are on the same environment, possible options: keep same, have different stepcounts for each, have one higher for both together - leave the same\n",
    " - what if uvf go to actual goal - doesnt matter\n",
    "\n",
    "monthly master meeting\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
