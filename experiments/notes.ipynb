{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talking points:\n",
    " - should I use your RND for the combined or stick with epsilon? we said yes, but max said that epsilon greedy outperforms rnd in fourrooms so maybe just leave it?\n",
    " - weird version of stable baselines, use_amp, double_q, also runs abysmal on more envs than 1? \n",
    " - udqn 193-196 same in both ifs\n",
    " \n",
    "\n",
    "## Thesis points: \n",
    "   - should I explain?  \n",
    "     - CNN - not yet at least\n",
    "\n",
    "\n",
    "## NEVER FORGET\n",
    "  1. test all functions you made\n",
    "  1. sign of anything wrong, dont ignore it,  \n",
    "  1. write thesis report \n",
    "  1. read papers\n",
    "\n",
    "## Todos:\n",
    "  1. write thesis\n",
    "  1. run variations of epsilon and random walk so it shows improvement while same amount of random steps, \n",
    "  1. get max rnd to work\n",
    "  1. get max rnd to use randomWalk\n",
    "\n",
    "### Experiments:\n",
    "- [ ] initialize with optimal trajectory to figure out how much is exploration and how much is generalization \n",
    "  - just fill replay buffer with optimal trajectory (is in repo), otherwise use eval callback to figure out perentage of visited state and compare, if the lines are the same between tpdqn and normal dqn, means exploration is not the difference \n",
    "  - could also do a callback that keeps track of diversity of replaybuffer state\n",
    "\n",
    "## Pseudopseudocode\n",
    "### Rollout\n",
    " 1. episode starts \n",
    " 1. if random() < chance: continue; else goto 5. \n",
    " 1. sample buffer for states, choose highest RND value state x, \n",
    " 1. utilize uvf to go to state x, until value function doesn't increase further\n",
    " 1. use main agent to try to go to goal\n",
    "### Training\n",
    " 1. for main agent use standard dual dqn\n",
    " 1. for uvf utilize HER\n",
    "\n",
    "## New questions\n",
    " - How to improve: \n",
    "   - part of the problem is very limited amount of real steps, forcing it to go more steps at the start might help? didnt help\n",
    "   - forcing it to continue when both uvf values (most recent and old one) are negative\n",
    "   - neither position oriented nor only context drawn, matters\n",
    "   - exploration of uvf currently is max(exploration_rate of agen, 0.2)\n",
    "\n",
    "\n",
    "## Answered Questions: \n",
    "\n",
    " - How many states, (6240 prob not as teleport reaches 119%): doesnt really matter\n",
    " - stacking states is fine? (turn 4,9,9 into 8,9,9 for uvf)? any ideas for good parameters? maybe 64 channels, but thats it\n",
    " - reward function is fine?  yeah\n",
    " - Why is the reward not discounted? just cause succes more important than optimilatiy in our case\n",
    " - Is giving uvf a done and +1 reward if obs==goal good idea? can do, should use the reward\n",
    " - how to implement experiment discussed last week?  - not sure yet\n",
    " - can the idea of 2 networks be called a weird version of actor critic? (for reading/implementation purposes) \n",
    " - should uvf and main have the same buffer? (is it even feasible), 2 buffer would be easier, merging to one makes more sense when merging networks to one,\n",
    "   uvf should use both, main agent only itself, q functions on everything\n",
    " - should I even keep the chance thingy or for starters just do 100% chance runs? 0,50,100\n",
    " - should I rather extract target as coordinates, or just the corresponding layer (as one layer of input is just position)? also should I alter the input in this case?\n",
    " - should I change the max timesteps? as both targets are on the same environment, possible options: keep same, have different stepcounts for each, have one higher for both together - leave the same\n",
    " - what if uvf go to actual goal - doesnt matter\n",
    "\n",
    "monthly master meeting\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
