{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# -------------------------\n",
    "# 8 days left\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "### Talking points:\n",
    "\n",
    " - discuss bughunt, maybe go over intrinsicRewardWalk, especially if actions chosen correctly in policy,\n",
    " - found bug I think, sometimes crashes on randomWalk with seed 16, but crashes run, doesnt continue with bad performance\n",
    " - show hyperTest folder pictures\n",
    " - should I describe what I am experimenting with in methodology or better to only leave in experiments? otherwise its kinda double  \n",
    " - should I add a discussion or just cover everything in conclusion? future work part of conclusion or little add on section? \n",
    " - glossary?\n",
    " - what other experiments to run? \n",
    " - hypernetwork in background if only used in 1 experiment? or should I somwhat explain it in the experiments section? \n",
    " \n",
    "\n",
    "### Bughuntnotes: \n",
    "  - randomWalk:\n",
    "     - seed 16 sometimes buggy (set np to 16 iterate over other ones)\n",
    "     - seed 26 worse than usual but not terrible\n",
    "     - FINALLY SEED 68 IS FORKED, RERUNNING TO DOUBLE CHECK\n",
    "     - rn 74 was forked\n",
    "     - 104 for heatmap comparison over time\n",
    "     - good run: https://wandb.ai/felix-kaubek/thesis/runs/zucptbk6?nw=nwuserfelixkaubek\n",
    "     - bad run: https://wandb.ai/felix-kaubek/thesis/runs/uxn6vjti?nw=nwuserfelixkaubek\n",
    "\n",
    "\n",
    "### Thesis what missing by section: (how many pages missing)\n",
    "Current prioritylist: \n",
    "  conclusion -> experiments -> related works -> background -> appendix -> methodology -> intro\n",
    "\n",
    " 0. general:\n",
    "    - no lose ends, czekovhs gun, <- focus on at the end\n",
    "    - double check everything to call pure exploration that, \n",
    "    - include graphics\n",
    "    - more citations\n",
    " 1. intro: \n",
    "    - more subsections and improved structure \n",
    "    - streamlining intro, less vague, higher understanding expected,\n",
    " 2. background: (1-2p)\n",
    "    - more formalization in background, especially in generalization\n",
    "    - flesh out a bit more \n",
    " 1. related works: (2p)\n",
    "    - fill out more, more citations \n",
    " 1. Methodology: (1-2p)\n",
    "    - environment & test sets \n",
    "    - more citations\n",
    "    - improve pseudocode\n",
    " 1. Experiments: (1-2p)\n",
    "    - write out all the bullet points\n",
    "    - structuring needs overhaul\n",
    " 1. Conclusion: (2p)\n",
    "    - future work completely missing atm\n",
    "    - extend & flesh out \n",
    "  \n",
    " A. Appendix:\n",
    "   - tables \n",
    "   - everything that didnt fit thematically story wise or imporancewise in the rest\n",
    "\n",
    "## Exp run:\n",
    " - Main comparison: \n",
    "    - 500k \n",
    "    - 50k \n",
    " - Hyper test: \n",
    "    - Tp chance for TP\n",
    "    - Tp Tournament size\n",
    "    - Hergo Tournament Size\n",
    "    - Hergo UVF archtitecture \n",
    "    - Hergo MaxSteps\n",
    "    - RandomWalk Steps \n",
    "\n",
    "\n",
    "## NEVER FORGET\n",
    "  1. test all functions you made\n",
    "  1. sign of anything wrong, dont ignore it,  \n",
    "  1. 9/11\n",
    "  1. write thesis report \n",
    "  1. read papers\n",
    "\n",
    "## Todos:\n",
    "\n",
    "  1. check in on experiments on delftblue -> visualize current experiments\n",
    "  1. write thesis -> focus on fleshing out story and including graphs\n",
    "\n",
    "  1. look into instability of training for randomWalk (also intriinsic reward)\n",
    "  1. bughunting -> bugfixing\n",
    "\n",
    "  1. graph updates: order of mentions, better naming, change x axis to k scale, rename legend, make lines in legend more visible \n",
    "  1. add goal and starting pos to heatmap, also look into intrinsic reward, \n",
    "\n",
    "  1. further experments to enrich thesis:  \n",
    "      1. possibly run rwf/irwf with onpolicy,  \n",
    "      1. run variations of epsilon and random walk so it shows improvement while same amount of random steps,   \n",
    "      1. run hyperparameter tuning for all methods (also baseline)    \n",
    "      1. sensitvity plotting of hyperparameter  - in work                                        \n",
    "\n",
    "\n",
    "\n",
    "## Answered Questions: \n",
    "\n",
    " - How many states, (6240 prob not as teleport reaches 119%): doesnt really matter\n",
    " - stacking states is fine? (turn 4,9,9 into 8,9,9 for uvf)? any ideas for good parameters? maybe 64 channels, but thats it\n",
    " - reward function is fine?  yeah\n",
    " - Why is the reward not discounted? just cause succes more important than optimilatiy in our case\n",
    " - Is giving uvf a done and +1 reward if obs==goal good idea? can do, should use the reward\n",
    " - how to implement experiment discussed last week?  - not sure yet\n",
    " - can the idea of 2 networks be called a weird version of actor critic? (for reading/implementation purposes) \n",
    " - should uvf and main have the same buffer? (is it even feasible), 2 buffer would be easier, merging to one makes more sense when merging networks to one,\n",
    "   uvf should use both, main agent only itself, q functions on everything\n",
    " - should I even keep the chance thingy or for starters just do 100% chance runs? 0,50,100\n",
    " - should I rather extract target as coordinates, or just the corresponding layer (as one layer of input is just position)? also should I alter the input in this case?\n",
    " - should I change the max timesteps? as both targets are on the same environment, possible options: keep same, have different stepcounts for each, have one higher for both together - leave the same\n",
    " - what if uvf go to actual goal - doesnt matter\n",
    "\n",
    " - many exp: \n",
    "   - arch: very similar, hypernetwork just takes a bit longer\n",
    "   - runtime: volatility of 500k is much higher? idk why\n",
    "   - comparison: test on 20 runs seem much better and cleaner, everything more stable, also uses more frequent test sampling, \n",
    "   - tournamentSize:\n",
    "     - tp: no difference really \n",
    "     - hergo: 1 is bad, everything else better, 1 might just have higher volatility\n",
    "\n",
    "old.\n",
    " - what are high memory nodes for? - smthing\n",
    " - how long should thesis be? kinda feels like sometimes just writing things to write things - with figures: 30-60 pages \n",
    " - should i do experiments with on policy (for rwf/irwf)? possible extension \n",
    " - names for methods: RandomWalk, IntrinsicRewardWalk,Teleportation and most imporantly Hergo? (cause influenced by go explore and relies on HER) \n",
    " - instead: pure exploration: epsilon based and intrinsic reward based, (shallow and deep), GoGo, \n",
    "\n",
    "monthly master meeting\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
