{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEVER FORGET\n",
    "  1. test all functions you made\n",
    "  1. sign of anything wrong, dont ignore it,  \n",
    "  1. write thesis report \n",
    "  1. read papers\n",
    "\n",
    "## Todos:\n",
    "  1. maybe rnd exploration\n",
    "  1. write thesis\n",
    "  1. include zero shot in thesis background\n",
    "  1. implement method that just goes somewhere before exploiting (random) as comparison\n",
    "  1. adapt buffers to actually handle vecenv\n",
    "  1. maybe add a tracker how many steps are taken by the uvf (maybe even with max_steps)\n",
    "\n",
    "### Experiments:\n",
    "- [ ] initialize with optimal trajectory to figure out how much is exploration and how much is generalization \n",
    "  - just fill replay buffer with optimal trajectory (is in repo), otherwise use eval callback to figure out perentage of visited state and compare, if the lines are the same between tpdqn and normal dqn, means exploration is not the difference \n",
    "  - could also do a callback that keeps track of diversity of replaybuffer state\n",
    "\n",
    "## Pseudopseudocode\n",
    "### Rollout\n",
    " 1. episode starts \n",
    " 1. if random() < chance: continue; else goto 5. \n",
    " 1. sample buffer for states, choose highest RND value state x, \n",
    " 1. utilize uvf to go to state x, until value function doesn't increase further\n",
    " 1. use main agent to try to go to goal\n",
    "### Training\n",
    " 1. for main agent use standard dual dqn\n",
    " 1. for uvf utilize HER\n",
    "\n",
    "## New questions\n",
    "\n",
    " - How many states, (6240 prob not as teleport reaches 119%)\n",
    " - stacking states is fine? (turn 4,9,9 into 8,9,9 for uvf)? any ideas for good parameters?\n",
    " - reward function is fine? \n",
    "\n",
    "## Answered Questions: \n",
    "\n",
    " - Why is the reward not discounted? just cause succes more important than optimilatiy in our case\n",
    " - Is giving uvf a done and +1 reward if obs==goal good idea? can do, should use the reward\n",
    " - how to implement experiment discussed last week?  - not sure yet\n",
    " - can the idea of 2 networks be called a weird version of actor critic? (for reading/implementation purposes) \n",
    " - should uvf and main have the same buffer? (is it even feasible), 2 buffer would be easier, merging to one makes more sense when merging networks to one,\n",
    "   uvf should use both, main agent only itself, q functions on everything\n",
    " - should I even keep the chance thingy or for starters just do 100% chance runs? 0,50,100\n",
    " - should I rather extract target as coordinates, or just the corresponding layer (as one layer of input is just position)? also should I alter the input in this case?\n",
    " - should I change the max timesteps? as both targets are on the same environment, possible options: keep same, have different stepcounts for each, have one higher for both together - leave the same\n",
    " - what if uvf go to actual goal - doesnt matter\n",
    "\n",
    "monthly master meeting\n",
    "\n",
    "To run: (buffer,tp_chance,rn) for eps=1.0\n",
    " - (500,0.0,0),(500,0.0,1),(500,1.0,3)\n",
    " - (10,0.0,4)\n",
    " - (50,0.5,0),(50,1.0,4)\n",
    " everything (start with buffer size 10&500) for eps 1.0\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
