{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talking points:\n",
    " - uvf itself works now \n",
    " - still much better with 500k buffer size, \n",
    " - -x uvf is uvf without reward at goal (simulating HERGO sitatuation)\n",
    " - randomSteps comparisons test/graphs should I try to find values so both of them align (randomSteps also uses epsilon greedy, but should probably do less)\n",
    " - should I use your RND for the combined or stick with epsilon? yes \n",
    " - tests have shown that stacking works much better than hypernetwork\n",
    " - \n",
    " - thesis text: \n",
    "   - how to write related research, should I summarize papers? or just say what they are generally about?\n",
    "   - was told related papers should be reasoned why not included as benchmark? how about max paper? \n",
    "   - should I write pseudocode for teleportation or other methods? \n",
    "   - should I explain?  \n",
    "     - CNN - not yet at least\n",
    "   - which experiments to include in main experiments section? \n",
    "\n",
    "\n",
    "## NEVER FORGET\n",
    "  1. test all functions you made\n",
    "  1. sign of anything wrong, dont ignore it,  \n",
    "  1. write thesis report \n",
    "  1. read papers\n",
    "\n",
    "## Todos:\n",
    "  1. write thesis\n",
    "  1. look into decaying performance of goal oriented algorithms\n",
    "  1. look into buffersizes of UVF's (goal oriented algs)\n",
    "  1. look into on-policy HER algorithms\n",
    "  1. goal conditioned paper research\n",
    "  1. randomwalk with different head of dqn outputting intrinsic exploration?? rnd implementation bascially, either sharing CNN or not, both possible\n",
    "  1. include counter of epsilon steps (showcasing that both have the same number of \"random\" steps) - todo: run\n",
    "  1. learn both q and u value so during rollout you only have to select the \n",
    "\n",
    "\n",
    "### Experiments:\n",
    "- [ ] initialize with optimal trajectory to figure out how much is exploration and how much is generalization \n",
    "  - just fill replay buffer with optimal trajectory (is in repo), otherwise use eval callback to figure out perentage of visited state and compare, if the lines are the same between tpdqn and normal dqn, means exploration is not the difference \n",
    "  - could also do a callback that keeps track of diversity of replaybuffer state\n",
    "\n",
    "## Pseudopseudocode\n",
    "### Rollout\n",
    " 1. episode starts \n",
    " 1. if random() < chance: continue; else goto 5. \n",
    " 1. sample buffer for states, choose highest RND value state x, \n",
    " 1. utilize uvf to go to state x, until value function doesn't increase further\n",
    " 1. use main agent to try to go to goal\n",
    "### Training\n",
    " 1. for main agent use standard dual dqn\n",
    " 1. for uvf utilize HER\n",
    "\n",
    "## New questions\n",
    " - How to improve: \n",
    "   - part of the problem is very limited amount of real steps, forcing it to go more steps at the start might help? didnt help\n",
    "   - forcing it to continue when both uvf values (most recent and old one) are negative\n",
    "   - neither position oriented nor only context drawn, matters\n",
    "   - exploration of uvf currently is max(exploration_rate of agen, 0.2)\n",
    "\n",
    "\n",
    "## Answered Questions: \n",
    "\n",
    " - How many states, (6240 prob not as teleport reaches 119%): doesnt really matter\n",
    " - stacking states is fine? (turn 4,9,9 into 8,9,9 for uvf)? any ideas for good parameters? maybe 64 channels, but thats it\n",
    " - reward function is fine?  yeah\n",
    " - Why is the reward not discounted? just cause succes more important than optimilatiy in our case\n",
    " - Is giving uvf a done and +1 reward if obs==goal good idea? can do, should use the reward\n",
    " - how to implement experiment discussed last week?  - not sure yet\n",
    " - can the idea of 2 networks be called a weird version of actor critic? (for reading/implementation purposes) \n",
    " - should uvf and main have the same buffer? (is it even feasible), 2 buffer would be easier, merging to one makes more sense when merging networks to one,\n",
    "   uvf should use both, main agent only itself, q functions on everything\n",
    " - should I even keep the chance thingy or for starters just do 100% chance runs? 0,50,100\n",
    " - should I rather extract target as coordinates, or just the corresponding layer (as one layer of input is just position)? also should I alter the input in this case?\n",
    " - should I change the max timesteps? as both targets are on the same environment, possible options: keep same, have different stepcounts for each, have one higher for both together - leave the same\n",
    " - what if uvf go to actual goal - doesnt matter\n",
    "\n",
    "monthly master meeting\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
